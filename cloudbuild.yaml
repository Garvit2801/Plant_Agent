# cloudbuild.yaml
timeout: "1200s"

substitutions:
  _REGION: asia-south2
  _REGION_SCHED: asia-south1
  _REPO: plant-agent
  _IMAGE: agent
  _SERVICE: plant-agent
  _BQ_LOCATION: asia-south2

steps:
  # 0) (Optional) show versions and auth
  - id: env-check
    name: gcr.io/google.com/cloudsdktool/cloud-sdk
    entrypoint: bash
    args:
      - -ceu
      - |
        echo "PROJECT_ID=$PROJECT_ID"
        gcloud --version
        bq version || true

  # 1) Build image
  - id: build
    name: gcr.io/cloud-builders/docker
    args:
      - build
      - -t
      - asia-south2-docker.pkg.dev/$PROJECT_ID/${_REPO}/${_IMAGE}:v$BUILD_ID
      - .

  # 2) Push image
  - id: push
    name: gcr.io/cloud-builders/docker
    args:
      - push
      - asia-south2-docker.pkg.dev/$PROJECT_ID/${_REPO}/${_IMAGE}:v$BUILD_ID

  # 3) Apply BigQuery SQL (tables/UDF/views)
  #    Runs files under sql/ddl then sql/views in lexical order.
  - id: bq-apply-sql
    name: gcr.io/google.com/cloudsdktool/cloud-sdk
    entrypoint: bash
    args:
      - -ceu
      - |
        set -euo pipefail
        PROJECT_ID="${PROJECT_ID:?}"
        BQ_LOCATION="${_BQ_LOCATION}"
        echo "Applying SQL to project=$PROJECT_ID location=$BQ_LOCATION"

        # Ensure dataset exists
        bq --location="$BQ_LOCATION" --project_id="$PROJECT_ID" mk --dataset plant_ops || true

        run_dir () {
          local dir="$1"
          if [[ -d "$dir" ]]; then
            for f in $(ls -1 "$dir"/*.sql 2>/dev/null | sort); do
              echo ">>> bq apply: $f"
              bq --location="$BQ_LOCATION" --project_id="$PROJECT_ID" query --nouse_legacy_sql < "$f"
            done
          fi
        }

        # Order: DDL (tables, UDFs) → views
        run_dir sql/ddl
        run_dir sql/views

  # 4) Deploy to Cloud Run (inject table envs so service logs cleanly)
  - id: deploy
    name: gcr.io/google.com/cloudsdktool/cloud-sdk
    entrypoint: gcloud
    args:
      - run
      - deploy
      - ${_SERVICE}
      - --image=asia-south2-docker.pkg.dev/$PROJECT_ID/${_REPO}/${_IMAGE}:v$BUILD_ID
      - --region=${_REGION}
      - --allow-unauthenticated
      - --platform=managed
      - --quiet
      - --set-env-vars=USE_MOCK=1,MOCK_TICK_SEC=5,APPLY_ENABLED=1,PROJECT_ID=$PROJECT_ID,BQ_LOCATION=${_BQ_LOCATION},BQ_MODEL_NAME=spower_reg,BQ_SNAPSHOTS_TABLE=$PROJECT_ID.plant_ops.snapshots,BQ_PLANS_TABLE=$PROJECT_ID.plant_ops.plans_v2,BQ_ACTUATIONS_TABLE=$PROJECT_ID.plant_ops.actuations_v2,BQ_ROUTINE_TABLE=$PROJECT_ID.plant_ops.routine_suggestions_v2

  # 5) Smoke tests (ingress-aware + readiness retry; /predict non-fatal if BQ not ready)
  - id: smoke-tests
    waitFor: ["deploy"]
    name: gcr.io/google.com/cloudsdktool/cloud-sdk:slim
    entrypoint: bash
    args:
      - -ceu
      - |
        set -euo pipefail

        region="${_REGION}"
        service="${_SERVICE}"

        base_url="$(gcloud run services describe "$service" --region="$region" --format='value(status.url)')"
        ingress="$(gcloud run services describe "$service" --region="$region" --format='value(metadata.annotations["run.googleapis.com/ingress"])')"

        echo "base_url=$base_url"
        echo "ingress=$ingress"

        if [[ -z "$base_url" ]]; then
          echo "❌ base_url empty (check service/region and run.viewer on the Cloud Build SA)"
          exit 1
        fi

        # If not publicly reachable from Cloud Build, skip HTTP tests.
        if [[ "$ingress" == "internal" || "$ingress" == "internal-and-cloud-load-balancing" ]]; then
          echo "ℹ️ ingress='$ingress' → skipping HTTP smoke tests."
          exit 0
        fi

        # Helper: return HTTP code
        get_code() { curl -sS -o /dev/null -w '%{http_code}' "$1" || echo "000"; }

        # Readiness retry for /health (unauth → try OIDC)
        auth_header=""
        tries=60    # up to ~5 minutes (60 * 5s)
        while (( tries-- )); do
          code="$(get_code "$base_url/health")"
          if [[ "$code" == "200" ]]; then break; fi
          if [[ -z "$auth_header" ]]; then
            id_token="$(gcloud auth print-identity-token --audiences="$base_url")" || true
            [[ -n "${id_token:-}" ]] && auth_header="Authorization: Bearer $id_token"
          fi
          if [[ -n "$auth_header" ]]; then
            code="$(curl -sS -o /dev/null -w '%{http_code}' -H "$auth_header" "$base_url/health" || true)"
            [[ "$code" == "200" ]] && break
          fi
          sleep 5
        done

        if [[ "$code" != "200" ]]; then
          echo "❌ /health not ready (last code=$code). Body (if auth available):"
          [[ -n "$auth_header" ]] && curl -sS -H "$auth_header" "$base_url/health" || curl -sS "$base_url/health" || true
          exit 1
        fi
        echo "✅ /health OK"

        # /debug/config → decide /ingest
        dbg="$(curl -sS ${auth_header:+-H "$auth_header"} "$base_url/debug/config" || true)"
        echo "debug/config: $dbg"
        bq_enabled="$(printf '%s' "$dbg" | grep -o '"bq_enabled":[[:space:]]*\(true\|false\)' | awk -F: '{gsub(/[[:space:]]*/,"",$2); print $2}')"

        # /ingest (only if BigQuery enabled by service)
        if [[ "$bq_enabled" == "true" ]]; then
          echo "Running /ingest"
          code="$(curl -sS -o /tmp/ingest.json -w '%{http_code}' -H "Content-Type: application/json" ${auth_header:+-H "$auth_header"} -X POST "$base_url/ingest" -d '{}' || true)"
          body="$(cat /tmp/ingest.json || true)"
          echo "ingest response ($code): $body"
          if [[ "$code" == "200" ]] && echo "$body" | grep -q '"ok":[[:space:]]*true'; then
            echo "✅ /ingest OK"
          else
            echo "⚠️  /ingest not ok:true — continuing (tables/model may not exist yet)."
          fi
        else
          echo "ℹ️ BigQuery disabled → skipping /ingest"
        fi

        # /predict/spower (non-fatal if BQ model not ready)
        echo "Running /predict/spower"
        pred_payload='{"snapshot":{"production_tph":120,"kiln_feed_tph":118,"separator_dp_pa":450,"id_fan_flow_Nm3_h":180000,"cooler_airflow_Nm3_h":200000,"kiln_speed_rpm":4.5,"o2_percent":2.1}}'
        pcode="$(curl -sS -o /tmp/predict.json -w '%{http_code}' -H "Content-Type: application/json" ${auth_header:+-H "$auth_header"} -X POST "$base_url/predict/spower" -d "$pred_payload" || true)"
        pbody="$(cat /tmp/predict.json || true)"
        echo "predict response ($pcode): $pbody"

        if [[ "$pcode" == "200" ]] && echo "$pbody" | grep -q '"predicted_specific_power_kwh_per_ton"'; then
          echo "✅ /predict/spower OK"
        else
          echo "⚠️  /predict/spower did not return prediction (likely BQ model missing). Not failing the build."
        fi

        echo "✅ Smoke tests completed."

  # 6) Ensure hourly Cloud Scheduler job (idempotent) + grant invoker
  - id: ensure-scheduler
    waitFor: ["smoke-tests"]
    name: gcr.io/google.com/cloudsdktool/cloud-sdk
    entrypoint: bash
    args:
      - -ceu
      - |
        set -euo pipefail
        REGION_RUN="${_REGION}"
        REGION_SCHED="${_REGION_SCHED}"
        SERVICE="${_SERVICE}"
        PROJECT_ID="${PROJECT_ID:?}"

        BASE_URL="$(gcloud run services describe "$SERVICE" --region="$REGION_RUN" --format='value(status.url)')"
        [[ -z "$BASE_URL" ]] && { echo "No base URL for service $SERVICE"; exit 1; }

        # Derive scheduler SA (or set via substitution if you prefer)
        SCHED_SA="plant-agent-scheduler@${PROJECT_ID}.iam.gserviceaccount.com"

        # Allow Scheduler SA to get OIDC tokens and invoke the service
        gcloud iam service-accounts add-iam-policy-binding "$SCHED_SA" \
          --member="serviceAccount:$SCHED_SA" \
          --role="roles/iam.serviceAccountTokenCreator" --project "$PROJECT_ID" || true

        gcloud run services add-iam-policy-binding "$SERVICE" \
          --region="$REGION_RUN" \
          --member="serviceAccount:$SCHED_SA" \
          --role="roles/run.invoker" || true

        JOB="plant-agent-routine-hourly"
        BODY='{"apply_top":true,"log_suggestions":true,"constraints":{"o2_percent":{"min":2.3,"max":4.5}}}'

        # Create or update the job to hit /optimize/routine hourly with OIDC
        if gcloud scheduler jobs describe "$JOB" --location="$REGION_SCHED" >/dev/null 2>&1; then
          echo "Updating existing scheduler job $JOB"
          gcloud scheduler jobs update http "$JOB" \
            --location="$REGION_SCHED" \
            --schedule="0 * * * *" \
            --uri="${BASE_URL}/optimize/routine" \
            --http-method=POST \
            --oidc-service-account-email="$SCHED_SA" \
            --oidc-token-audience="$BASE_URL" \
            --headers=Content-Type=application/json \
            --message-body="$BODY"
        else
          echo "Creating scheduler job $JOB"
          gcloud scheduler jobs create http "$JOB" \
            --location="$REGION_SCHED" \
            --schedule="0 * * * *" \
            --uri="${BASE_URL}/optimize/routine" \
            --http-method=POST \
            --oidc-service-account-email="$SCHED_SA" \
            --oidc-token-audience="$BASE_URL" \
            --headers=Content-Type=application/json \
            --message-body="$BODY"
        fi

images:
  - asia-south2-docker.pkg.dev/$PROJECT_ID/${_REPO}/${_IMAGE}:v$BUILD_ID

options:
  defaultLogsBucketBehavior: REGIONAL_USER_OWNED_BUCKET
  logging: CLOUD_LOGGING_ONLY
